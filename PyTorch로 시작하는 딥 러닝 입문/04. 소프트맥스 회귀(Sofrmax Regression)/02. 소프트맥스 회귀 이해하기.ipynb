{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. 다중 클래스 분류(Multi-class Classfication)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 개 이상의 답 중 하나를 고르는 것을 다중 클래스 분류라고 합니다. 아래의 문제는 꽃받침의 길이와 넓이, 꽃잎의 길이와 넓이라는 4개의 특성으로 세 개의 붓꽃 품종 중 어떤 품종인지 예측하는 전형적인 다중 클래스 분류 문제입니다. \n",
    "\n",
    "|Sepal Length (cm, x_1)|Sepal Width (cm, x_2)|Petal Length (cm, x_3)|Petal width (cm, x_4)|Species (y)|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|5.1|3.5|1.4|0.2|setosa|\n",
    "|4.9|3.0|1.4|0.2|setosa|\n",
    "|5.8|2.6|4.0|1.2|versicolor|\n",
    "|6.7|3.0|5.2|2.3|virginica|\n",
    "|5.6|2.8|4.9|2.0|virginica|\n",
    "\n",
    "위 붓꽃 품종 분류 문제를 어떻게 풀지 고민하기 앞서 로지스틱 회귀의 이진 분류를 복습해보겠습니다.\n",
    "\n",
    "이번 챕터에선 아래와 같은 notation을 사용합니다.\n",
    "- 입력: $X$\n",
    "- 가중치: $W$\n",
    "- 편향: $B$\n",
    "- 출력: $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 로지스틱 회귀\n",
    "\n",
    "로시스틱 회귀에서 시그모이드 함수는 예측값을 0과 1 사이의 값으로 만듭니다. 예를 들어 스팸 메일 분류기를 로지스틱 회귀로 구현했을 때, 출력이 0.75면 스팸 메일일 확률이 75%라는 의미가 된다.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://wikidocs.net/images/page/59427/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG\">\n",
    "</center>\n",
    "\n",
    "$$H(X) = sigmoid(WX+B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. 소프트맥스 함수**\n",
    "---\n",
    "\n",
    "소프트맥스 함수는 분류해야하는 클래스의 총 개수를 k개라 했을 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정합니다.\n",
    "\n",
    "## 2.1 소프트맥스 함수의 이해\n",
    "k차원의 벡터에서 $i$번째 원소를 $z_i$, $i$번째 클래스가 정답일 확률을 $p_i$로 나타낸다고 할 때, 소프트맥스 함수는 $p_i$를 다음과 같이 정의합니다.\n",
    "\n",
    "$$p_i={e^{z_i}\\over \\sum_{j=1}^k e^{z_j}} \\; for\\;1,2,\\cdots,k$$\n",
    "\n",
    "붓꽃 문제의 경우, k = 3이므로 3차원 벡터 $z=[z_1,z_2,z_3]$의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 리턴합니다.\n",
    "\n",
    "### $$softmax(z) = \\Big[ {e^{z_1}\\over \\sum_{j=1}^3 e^{z_j}} {e^{z_2}\\over \\sum_{j=1}^3 e^{z_j}} {e^{z_3}\\over \\sum_{j=1}^3 e^{z_j}} \\Big] = [p_1, p_2, p_3] = \\hat{y} = 예측값$$\n",
    "\n",
    "$p_1, p_2, p_3$ 각각은 1번 클래스가 정답일 확률, 2번 클래스가 정답일 확률, 3번 클래스가 정답일 확률을 나타내며 0과 1 사이의 값으로 총합은 1이 된다. 이것을 붓꽃의 예시에 적용하면 \n",
    "\n",
    "### $$softmax(z) = \\Big[ {e^{z_1}\\over \\sum_{j=1}^3 e^{z_j}} {e^{z_2}\\over \\sum_{j=1}^3 e^{z_j}} {e^{z_3}\\over \\sum_{j=1}^3 e^{z_j}} \\Big] = [p_1, p_2, p_3] = [p_{virginica}, p_{setosa}, p_{versicolor}]$$\n",
    "\n",
    "이다. 순서는 무작위 선택일뿐 의미는 없다.\n",
    "\n",
    "\n",
    "## 2.2 그림을 통한 이해\n",
    "<img src=\"https://wikidocs.net/images/page/35476/softmax1_final_final.PNG\">\n",
    "\n",
    "위의 그림에서 점차 살을 붙여볼 것이다. 여기선 샘플 데이터를 1개씩 입력받아 처리한다고 가정합시다. 즉, 배치 크기가 1입니다.\n",
    "\n",
    "위 그림에선 하나의 샘플 데이터는 4개의 독립 변수 $x$를 가집니다. 이는 모델이 4차원 벡터를 입력받는 것으로 이해할 수 있습니다. 그런데 분류하고자 하는 클래스는 3개이므로 어떤 가중치 연산을 통해 3차원 벡터로 변환되어야 합니다. 위 그림에선 $z$로 표현했습니다.\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/35476/softmaxbetween1and2.PNG\">\n",
    "\n",
    "위와 같은 방법으로 샘플 데이터 벡터를 소프트맥스 함수의 입력 차원으로 축소합니다. 위 그림의 화살표는 총 12개이며 전부 다른 가중치를 가지고 학습 과정에서 점차 오차를 최소하하는 가중치로 값이 변경됩니다.\n",
    "\n",
    "\n",
    "오차 계산을 위해선 먼저 실제값이 있어야 합니다. 소프트맥스 함수에선 실제값을 원-핫 벡터로 표현합니다.\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/35476/softmax2_final.PNG\">\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/35476/softmax4.PNG\">\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/35476/softmax6_final.PNG\">\n",
    "\n",
    "그리고 실제값과 비교하며 가중치와 편향을 업데이트 해주는 방식입니다. 비용 함수는 뒤에서 자세히 나옵니다.\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/59427/%EA%B0%80%EC%84%A4.PNG\">\n",
    "\n",
    "소프트맥스 회귀를 벡터와 행렬 연산으로 표현하면 다음과 같으며 이때, $f$는 특성의 수, $c$는 클래스의 개수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. 붓꽃 품종 분류하기를 행렬 연산으로 이해하기**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제 데이터는 전체 샘플의 개수가 5개, 특성이 4개이므로 $5 \\times 4$ 행렬 $X$로 정의합니다.\n",
    "\n",
    "\n",
    "$$\\begin{aligned}X &= \\begin{bmatrix}5.1&3.5&1.4&0.2\\\\4.9&3.0&1.4&0.2\\\\5.8&2.6&4.0&1.2\\\\6.7&3.0&5.2&2.3\\\\5.6&2.8&4.9&2.0\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}x_{11}&x_{12}&x_{13}&x_{14}\\\\x_{21}&x_{22}&x_{23}&x_{24}\\\\x_{31}&x_{32}&x_{33}&x_{34}\\\\x_{41}&x_{42}&x_{43}&x_{44}\\\\x_{51}&x_{52}&x_{53}&x_{54}\\end{bmatrix} \\end{aligned}$$\n",
    "\n",
    "\n",
    "이번 문제는 선택지가 3개인 문제이므로 가설의 예측값으로 얻는 행렬 $\\hat{Y}$의 열의 개수는 3개여야 합니다. 그리고 각 행은 행렬 $X$의 각 행의 예측값이므로 행의 크기는 동일해야 합니다. 결과적으로 행렬 $\\hat{Y}$의 크기는 $5 \\times 3$입니다.\n",
    "\n",
    "\n",
    "$$\\hat{Y}=\\begin{bmatrix}y_{11}&y_{12}&y_{13}\\\\y_{21}&y_{22}&y_{23}\\\\y_{31}&y_{32}&y_{33}\\\\y_{41}&y_{42}&y_{43}\\\\y_{51}&y_{52}&y_{53}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "행렬 $\\hat{Y}$가 $5 \\times 3$이고 행렬 $X$가 $5 \\times 4$이므로 가중치 행렬 $W$는 $4 \\times 3$의 크기를 가진 행렬이여야 한다.\n",
    "\n",
    "\n",
    "$$W = \\begin{bmatrix}w_{11}&w_{12}&w_{13}\\\\w_{21}&w_{22}&w_{23}\\\\w_{31}&w_{32}&w_{33}\\\\w_{41}&w_{42}&w_{43}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "편향 행렬 $B$는 예측값 행렬 $\\hat{Y}$와 크기가 동일해야 하므로 $5 \\times 3$이다.\n",
    "\n",
    "\n",
    "$$B=\\begin{bmatrix}b_{11}&b_{12}&b_{13}\\\\b_{21}&b_{22}&b_{23}\\\\b_{31}&b_{32}&b_{33}\\\\b_{41}&b_{42}&b_{43}\\\\b_{51}&b_{52}&b_{53}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "결과적으로 가설식은 다음과 같습니다.\n",
    "\n",
    "\n",
    "$$\\hat{Y}=softmax(XW+B) \\\\\n",
    "\\begin{bmatrix}y_{11}&y_{12}&y_{13}\\\\y_{21}&y_{22}&y_{23}\\\\y_{31}&y_{32}&y_{33}\\\\y_{41}&y_{42}&y_{43}\\\\y_{51}&y_{52}&y_{53}\\end{bmatrix} =\n",
    "softmax\\begin{pmatrix}\n",
    "    \\begin{bmatrix}x_{11}&x_{12}&x_{13}&x_{14}\\\\x_{21}&x_{22}&x_{23}&x_{24}\\\\x_{31}&x_{32}&x_{33}&x_{34}\\\\x_{41}&x_{42}&x_{43}&x_{44}\\\\x_{51}&x_{52}&x_{53}&x_{54}\\end{bmatrix} &\n",
    "    \\begin{bmatrix}w_{11}&w_{12}&w_{13}\\\\w_{21}&w_{22}&w_{23}\\\\w_{31}&w_{32}&w_{33}\\\\w_{41}&w_{42}&w_{43}\\end{bmatrix} &\n",
    "    + \\begin{bmatrix}b_{11}&b_{12}&b_{13}\\\\b_{21}&b_{22}&b_{23}\\\\b_{31}&b_{32}&b_{33}\\\\b_{41}&b_{42}&b_{43}\\\\b_{51}&b_{52}&b_{53}\\end{bmatrix}\n",
    "\\end{pmatrix})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. 비용 함수(Cost Function)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용합니다. \n",
    "\n",
    "## 4.1 크로스 엔트로피 함수\n",
    "아래에서 $y$는 실제값, $k$는 클래스의 개수로 정의합니다. $y_j$는 실제값 원-핫 벡터의 $j$번째 인덱스를 의미하며, $p_j$는 샘플 데이터가 $j$번째 클래스일 확률을 나타냅니다. \n",
    "\n",
    "$$cost(W)=-\\sum_{j=1}^k y_j \\log{p_j}$$\n",
    "\n",
    "c가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, $p_c=1$은 $\\hat{y}$가 $y$를 정확하게 예측한 경우가 됩니다. 이를 식에 대입해보면 $-1\\log(1)=0$이 되기 때문에, 결과적으로 $\\hat{y}$가 $y$를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 됩니다. 즉, $-\\sum_{j=1}^k y_j \\log{p_j}$을 최소화하는 방향으로 학습해야 합니다. \n",
    "\n",
    "이를 $n$개의 전체 데이터에 대한 평균을 구한다고 하면 최종 비용 함수는 다음과 같습니다.\n",
    "\n",
    "$$cost(W) = -{1\\over n}\\sum_{i=1}^n \\sum_{j=1}^k y_j^{(i)} \\log{p_j^{(i)}}$$\n",
    "\n",
    "\n",
    "\n",
    "## 4.2 이진 분류에서 크로스 엔트로피 함수\n",
    "로지스틱 회귀에서 배운 크로스 엔트로피 함수식과 달라보이지만, 본질적으로 동일한 함수식입니다. 로지스틱 회귀의 크로스 엔트로피 함수식으로부터 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출해봅시다.\n",
    "\n",
    "$$cost(W) = -(y\\log{H(X)} + (1 - y) \\log{(1 - H(X))})$$\n",
    "\n",
    "위의 식은 앞서 로지스틱 회귀에서 배웠던 크로스 엔트로피의 함수식을 보여줍니다. 위의 식에서 $y$를 $y_1$, $y-1$을 $y_2$로 치환하고, $H(X)$를 $p_1$, $1 - H(X)$를 $p_2$로 치환해봅시다. 결과적으로 아래의 식을 얻을 수 있습니다.\n",
    "\n",
    "$$-(y_1 \\log(p_1) + y_2 \\log(p_2))$$\n",
    "\n",
    "이 식은 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "$$-(\\sum_{i=1}^2 y_i \\log{p_i})$$\n",
    "\n",
    "소프트맥스 회귀에서는 $k$의 값이 고정된 값이 아니므로 2를 $k$로 변경합니다. \n",
    "\n",
    "$$-(\\sum_{i=1}^k y_i \\log{p_i})$$\n",
    "\n",
    "위의 식은 결과적으로 소프트맥스 회귀의 식과 동일합니다. 역으로 소프트맥스 회귀에서 로지스틱 회귀의 크로스 엔트로피함수식을 얻는 것은 $k$를 2로 하고, $y_1$과 $y_2$를 각각 $y$, $1-y$로 치환하고, $p_1$와 $p_2$를 각각 $H(X)$와 $1-H(X)$로 치환하면 됩니다. \n",
    "\n",
    "정리하면 소프트맥스 함수의 최종 비용 함수에서 $k$가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같습니다.\n",
    "\n",
    "$$cost(W) = -{1\\over n}\\sum_{i=1}^n \\sum_{j=1}^k y_j^{(i)} \\log{p_j^{(i)}} = -{1\\over n}\\sum_{i=1}^n [y^{(i)}\\log{(p^{(i)})} + (1 - y^{(i)}) \\log{(1 - p^{(i)})}]$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
