{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합을 막는 여러 방법이 있습니다. 그 중, 인공 신경망의 과적합을 막는 방법에 초점을 두겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 데이터의 양을 늘리기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합이 쉽게 발생합니다. 그렇기 때문에 데이터 양을 늘릴수록 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다. \n",
    "\n",
    "만약, 데이터 양이 적을 경우, 의도적으로 기존의 데이터를 조금씩 변형하여 추가하고 데이터의 양을 늘리기도 합니다. 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지의 경우, 데이터 증식이 많이 사용되는데 이미지를 회전하거나 노이즈를 추가하고 일부분을 수정하는 등으로 데이터를 증식시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 모델의 복잡도 줄이기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정됩니다. 이 복잡도를 줄이면 과적합을 어느 정도 해결할 수 있습니다.\n",
    "\n",
    "예를 들어 다음과 같이 클래스를 사용하여 구현한 인공 신경망이 있다고 가정해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 인공 신경망은 3개의 선형 레이어를 가지고 있습니다. 위 인공 신경망이 입력 데이터에 과적합 현상을 보인다면 다음과 같이 인공 신경망의 복잡도를 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 인공 신경망은 2개의 선형 레이어를 가지고 있습니다.\n",
    "\n",
    "> 인공 신경망에서 모델에 있는 매개변수들의 수를 모델의 수용력(capacity)이라고 하기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 가중치 규제(Regularization) 적용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 모델은 적은 수의 매개변수를 가진 모델을 말합니다. 반대로 복잡한 모델은 많은 수의 매개변수를 가진 모델입니다. 그리고 이는 과적합될 가능성이 높습니다. 이러한 복잡한 모델을 좀 더 간단하게 하는 방법으로 **가중치 규제(Regularization)**가 있습니다.\n",
    "\n",
    "- L1 규제: 가중치 w들의 절대값 합계를 비용 함수에 추가합니다.\n",
    "- L2 규제: 모든 가중치 w들의 제곱합을 비용 함수에 추가합니다.\n",
    "\n",
    "즉, L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\lambda \\lvert w \\rvert$를 더한 값을 비용 함수로 합니다. L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\frac{1}{2} \\lambda w^2$를 더한 값을 비용 함수로 사용합니다. $\\lambda$는 규제의 강도를 정하는 하이퍼파라미터입니다. $\\lambda$가 크다면 모델이 훈련 데이터에 대해서 규제를 더 우선 시 한다는 의미가 됩니다. \n",
    "\n",
    "이 두 방법 모두 비용 함수를 최소화하기 위해서 가중치 w들의 값이 작아져야 한다는 특징이 있습니다. L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 합니다. 이렇게 되면 가중치 w의 값들은 0이나 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용하지 않게 됩니다. 즉, 영향을 거의 못주는 특성이 있게 됩니다.\n",
    "\n",
    "L2 규제는 가중치들의 제곱을 최소화하므로 w의 값이 완전히 0이 되지 않고 0에 가까워지기만 합니다. 그렇기에 L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지 정확히 판단하고자 할 때 유용합니다. 만약 이런 판단이 필요없다면 경험적으로 L2 규제가 더 잘 작동하기에 L2 규제를 사용합니다. 인공 신경망에서 L2 규제는 **가중치 감쇠(weight decay)** 라고도 부릅니다.\n",
    "\n",
    "파이토치에서는 옵티마이저의 weight_decay 매개변수를 설정하므로 L2 규제를 적용합니다. weight_decay 매개변수의 기본값은 0입니다. weight_decay 매개변수에 다른 값을 설정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법입니다. \n",
    "\n",
    "예를 들어, 드롭아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용합니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/60751/%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.PNG\">\n",
    "\n",
    "드롭아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적입니다. 학습 시에는 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이지 않게 해주고 매번 랜덤 뉴런으로 선택하므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지합니다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
