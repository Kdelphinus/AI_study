{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 인공 신경망의 이해(Neural Network Overview)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 예제를 위해 사용될 인공 신경망을 먼저 알아봅시다. 역전파의 이해를 위해서 여기서 사용할 인공 신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가집니다. 또한 각 층마다 뉴런은 두 개씩 존재하며 모든 뉴런의 활성화 함수는 시그모이드 함수를 사용합니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/37406/nn1_final.PNG\">\n",
    "\n",
    "위의 그림은 여기서 사용할 인공 신경망의 모습을 보여줍니다. 은닉층과 출력층의 모든 뉴런에 변수 $z$가 존재하는데 이 변수는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미합니다. 이 $z$는 아직 활성화 함수인 시그모이드 함수를 거치지 않았습니다. $z$가 시그모이드 함수를 거치면 우측의 $|$를 지나 변수 $h, o$가 됩니다. 이는 각 뉴런의 출력값과도 같습니다. \n",
    "\n",
    "이번 역전파 예지는 인공 신경망에 존재하는 모든 가중치 $W$에 대해서 역전파를 통해 업데이트 하는 것을 목표로 합니다. 또한 해당 인공 신경망은 편향 $b$는 고려하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 순전파(Forward Propagation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/37406/nn2_final_final.PNG\">\n",
    "\n",
    "주어진 값이 위의 그림과 같을 때, 순전파를 진행해봅시다. 위의 그림에서 소수점 앞의 0은 생략되었습니다. 파란색 숫자는 입력값, 빨간색 숫자는 각 가중치의 값을 의미합니다. 앞으로 진행하는 결과값은 소수점 아래 여덟번째 자리까지 반올림하여 표기합니다.\n",
    "\n",
    "각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됩니다. $z_1, z_2$는 시그모이드 함수의 입력으로 사용되는 각각의 값에 해당됩니다.\n",
    "\n",
    "$$z_1 = W_1x_1 + W_2x_2 = 0.3 \\times 0.1 + 0.25 \\times 0.2 = 0.08 \\\\\n",
    "z_2 = W_3x_1 + W_4x_2 = 0.4 \\times 0.1 + 0.35 \\times 0.2 = 0.11$$\n",
    "\n",
    "그리고 $z_1, z_2$는 시그모이드 함수의 입력으로 사용되어 최종 출력값 $h_1, h_2$를 출력합니다.\n",
    "\n",
    "$$h_1 = sigmoid(z_1) = 0.51998934 \\\\\n",
    "h_2 = sigmoid(z_2) = 0.52747230$$\n",
    "\n",
    "그렇게 나온 출력값 $h_1$과 $h_2$는 다음 층인 출력층의 뉴런으로 향하여 해당되는 가중치와 곱해지고 가중합이 되어 출력층 뉴런의 시그모이드 함수의 입력값 $z_3, z_4$가 됩니다. \n",
    "\n",
    "$$z_3 = W_5h_1 + W_6h_2 = 0.45 \\times h_1 + 0.4 \\times h_2 = 0.44498412 \\\\\n",
    "z_4 = W_7h_1 + W_8h_2 = 0.7 \\times h_1 + 0.6 \\times h_2 = 0.68047592$$\n",
    "\n",
    "$z_3, z_4$가 출력층 뉴런에서 시그모이드 함수를 거쳐 나온 값 $o_1, o_2$는 이 인공 신경망이 최종적으로 계산한 출력값입니다. 예측값이라고도 부릅니다.\n",
    "\n",
    "$$o_1 = sigmoid(z_3) = 0.60944600 \\\\\n",
    "o_2 = sigmoid(z_4) = 0.66384491$$\n",
    "\n",
    "이제 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택해야 합니다. 이번엔 평균 제곱 오차 MSE를 사용합니다. 식에서는 실제값을 target, 예측값은 output이라고 표현하였습니다. 그리고 각 오차를 모두 더하면 전체 오차가 나옵니다.\n",
    "\n",
    "$$E_{o1} = \\frac{1}{2}(target_{o1} - output_{o1})^2 = 0.02193381 \\\\\n",
    "E_{o2} = \\frac{1}{2}(target_{o2} - output_{o2})^2 = 0.00203809 \\\\\n",
    "E_{total} = E_{o1} + E_{o2} = 0.02397190$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 역전파 1단계(BackPropagation Step 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파는 순전파와 반대로 출력층에서 입력층 방향으로 계산하며 가중치를 업데이트합니다. 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 업데이트하는 단계를 역전파 1단계, 그리고 N층과 N층 이전층 사이의 가중치를 업데이트하는 단계를 역전파 2단계라고 합니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/37406/nn3_final.PNG\">\n",
    "\n",
    "역전파 1단계에서 업데이트 해야 할 가중치는 $W_5, W_6, W_7, W_8$으로 총 4개입니다. 원리 자체는 모두에게 동일하기에 $W_5$에 대해 업데이트를 진행하겠습니다. 가중치 $W_5$를 업데이트 하기 위해서 $\\frac{\\partial E_{total}}{\\partial W_5}$를 계산해야 합니다. 이를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라 풀어쓸 수 있습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_5} \n",
    "= \\frac{\\partial E_{total}}{\\partial o_1}\n",
    "\\times \\frac{\\partial o_1}{\\partial z_3}\n",
    "\\times \\frac{\\partial z_3}{\\partial W_5}$$\n",
    "\n",
    "위의 식을 순서대로 계산해봅시다. 우선 첫번째 항에 대해서 계산해보겠습니다. 미분하기 전 $E_{total}$의 값은 앞서 순전파를 진행하고 계산했던 전체 오차값입니다.\n",
    "\n",
    "$$E_{total} = \\frac{1}{2}(target_{o1} - output_{o1})^2 + \\frac{1}{2}(target_{o2} - output_{o2})^2$$\n",
    "\n",
    "이에 $\\frac{\\partial E_{total}}{\\partial o_1}$는 다음과 같습니다.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial E_{total}}{\\partial o_1} &= 2 \\times \\frac{1}{2}(target_{o1} - output_{o1})^{2-1} \\times (-1) + 0 \\\\\n",
    "&= -(target_{o1} - output_{o1}) \\\\ \n",
    "&= -(0.4 - 0.60944600) \\\\\n",
    "&= 0.20944600 \n",
    "\\end{aligned}$$\n",
    "\n",
    "이제 두번째 항을 주목합시다. $o_1$은 시그모이드 함수의 출력값입니다. 그리고 시그모이드 함수의 미분은 $f(x)\\times (1 - f(x))$입니다. 이에 따라서 두번째 항의 미분 결과는 다음과 같습니다.\n",
    "\n",
    "$$\\frac{\\partial o_1}{\\partial z_3} = o_1 \\times (1 - o_1) = 0.60944600(1 - 0.60944600) = 0.23802157$$\n",
    "\n",
    "마지막으로 세번째 항은 $h_1$과 동일합니다.\n",
    "\n",
    "$$\\frac{\\partial z_3}{\\partial W_5} = h_1 = 0.51998934$$\n",
    "\n",
    "이제 구한 값을 모두 곱해주면 값을 구할 수 있습니다. \n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_5} = 0.20944600 \\times 0.23802157 \\times 0.51998934 = 0.02592286$$\n",
    "\n",
    "이제 경사 하강법을 통해 가중치를 업데이트 할 때입니다. 하이퍼파라미터에 해당되는 학습률 $\\alpha$는 0.5라고 가정합니다.\n",
    "\n",
    "$$W_5^{+} = W_5 - \\alpha \\frac{\\partial E_{total}}{\\partial W_5}\n",
    "= 0.45 - 0.5 \\times 0.02592286 = 0.43703857$$\n",
    "\n",
    "이와 같은 원리로 $W_6^+, W_7^+, W_8^+$을 계산할 수 있습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_6} \n",
    "= \\frac{\\partial E_{total}}{\\partial o_1}\n",
    "\\times \\frac{\\partial o_1}{\\partial z_3}\n",
    "\\times \\frac{\\partial z_3}{\\partial W_6}\n",
    "\\rightarrow W_6^+ = 0.38685205$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_7} \n",
    "= \\frac{\\partial E_{total}}{\\partial o_2}\n",
    "\\times \\frac{\\partial o_2}{\\partial z_4}\n",
    "\\times \\frac{\\partial z_4}{\\partial W_7}\n",
    "\\rightarrow W_7^+ = 0.69629578$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_8} \n",
    "= \\frac{\\partial E_{total}}{\\partial o_2}\n",
    "\\times \\frac{\\partial o_2}{\\partial z_4}\n",
    "\\times \\frac{\\partial z_4}{\\partial W_8}\n",
    "\\rightarrow W_8^+ = 0.59624247$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 역전파 2단계(BackPropagation Step 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/37406/nn4.PNG\">\n",
    "\n",
    "계속 이어서 입력층 방향으로 이동하며 계산을 이어갑니다. 위의 그림에서 빨간색 화살표는 순전파의 정반대 방향인 역전파의 방향을 보여줍니다. \n",
    "\n",
    "이번 단계에서 계산할 가중치는 $W_1, W_2, W_3, W_4$입니다. 원리 자체는 1단계와 동일하기에 바로 계산해보겠습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_1} \n",
    "= \\frac{\\partial E_{total}}{\\partial h_1}\n",
    "\\times \\frac{\\partial h_1}{\\partial z_1}\n",
    "\\times \\frac{\\partial z_1}{\\partial W_1}$$\n",
    "\n",
    "위의 식에서 $\\frac{\\partial E_{total}}{\\partial h_1}$은 다음과 같이 식을 풀어쓸 수 있습니다. \n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial h_1} = \\frac{\\partial E_{o1}}{\\partial h_1} + \\frac{\\partial E_{o2}}{\\partial h_1}$$\n",
    "\n",
    "위의 식의 우변을 구해보겠습니다. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial E_{o1}}{\\partial h_1} &= \n",
    "\\frac{\\partial E_{o1}}{\\partial z_3} \\times \n",
    "\\frac{\\partial z_3}{\\partial h_1}\n",
    "= \\frac{\\partial E_{o1}}{\\partial o_1} \\times \n",
    "\\frac{\\partial o_1}{\\partial z_3} \\times\n",
    "\\frac{\\partial z_3}{\\partial h_1} \\\\\n",
    "&= -(target_{o1} - output_{01}) \\times o_1 \\times (1 - o_1) \\times W_5 \\\\\n",
    "&= 0.20944600 \\times 0.23802157 \\times 0.45 = 0.02243370\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\frac{\\partial E_{o2}}{\\partial h_1} = \n",
    "\\frac{\\partial E_{o2}}{\\partial z_4} \\times \n",
    "\\frac{\\partial z_4}{\\partial h_1}\n",
    "= \\frac{\\partial E_{o2}}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial z_4} \\times\n",
    "\\frac{\\partial z_4}{\\partial h_1}\n",
    "= 0.00997311$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial h_1} = 0.02243370 + 0.00997311 = 0.03240681$$\n",
    "\n",
    "이제 첫 번째 항을 구했습니다. 나머지 두 항에 대해서도 구해보겠습니다.\n",
    "\n",
    "$$\\frac{\\partial h_1}{\\partial z_1} \n",
    "= h_1 \\times (1 - h_1)\n",
    "= 0.51998934(1 - 0.51998934)\n",
    "= 0.24960043$$\n",
    "\n",
    "$$\\frac{\\partial z_1}{\\partial W_1} = x_1 = 0.1$$\n",
    "\n",
    "즉, $\\frac{\\partial E_{total}}{\\partial W_1}$은 다음과 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_1} = 0.03240681 \\times 0.24960043 \\times 0.1 = 0.00080888$$\n",
    "\n",
    "이제 경사 하강법을 통해 가중치를 업데이트 할 수 있습니다.\n",
    "\n",
    "$$W_1^{+} = W_1 - \\alpha \\frac{\\partial E_{total}}{\\partial W_1}\n",
    "= 0.1 - 0.5 \\times 0.00080888 = 0.29959556$$\n",
    "\n",
    "이와 같은 원리로 $W_2^+, W_3^+, W_4^+$을 계산할 수 있습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_2} \n",
    "= \\frac{\\partial E_{total}}{\\partial h_1}\n",
    "\\times \\frac{\\partial h_1}{\\partial z_1}\n",
    "\\times \\frac{\\partial z_1}{\\partial W_2}\n",
    "\\rightarrow W_2^+ = 0.24919112$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_3} \n",
    "= \\frac{\\partial E_{total}}{\\partial h_2}\n",
    "\\times \\frac{\\partial h_2}{\\partial z_2}\n",
    "\\times \\frac{\\partial z_2}{\\partial W_3}\n",
    "\\rightarrow W_3^+ = 0.39964496$$\n",
    "\n",
    "$$\\frac{\\partial E_{total}}{\\partial W_4} \n",
    "= \\frac{\\partial E_{total}}{\\partial h_2}\n",
    "\\times \\frac{\\partial h_2}{\\partial z_2}\n",
    "\\times \\frac{\\partial z_2}{\\partial W_4}\n",
    "\\rightarrow W_4^+ = 0.34928991$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 결과 확인**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "업데이트 된 가중치에 대해서 다시 한 번 순전파를 진행하여 오차가 감소하였는지 확인해보겠습니다.\n",
    "\n",
    "$$z_1 = W_1x_1 + W_2x_2 = 0.29959556\\times0.1+0.24919112\\times0.2=0.07979778 \\\\\n",
    "z_2=W_3x_1+W_4x_2=0.39964496\\times0.1+0.34928991\\times0.2=0.10982248 \\\\\n",
    "h_1 = sigmoid(z_1) = 0.51993887 \\\\\n",
    "h_2 = sigmoid(z_2) = 0.52742806 \\\\\n",
    "z_3 = W_5h_1+W_5h_2=0.43703857\\times h_1+0.38685205\\times h_2 = 0.43126996 \\\\\n",
    "z_4 = W_7h_1 + W_8h_2 = 0.69629578 \\times h_1 + 0.59624247 \\times h_2 = 0.67650625 \\\\\n",
    "o_1 = sigmoid(z_3) = 0.60617688 \\\\\n",
    "o_2 = sigmoid(z_4) = 0.66295848 \\\\\n",
    "E_{o1} = \\frac{1}{2}(target_{o1} - output{o1})^2 = 0.02125445 \\\\\n",
    "E_{o2} = \\frac{1}{2}(target_{o2} - output{o2})^2 = 0.00198189 \\\\\n",
    "E_{total} = E_{o1} + E_{o2} = 0.02323634$$\n",
    "\n",
    "기존의 오차가 0.02397190이였으므로 1번의 역전파로 오차가 감소한 것을 확인할 수 있습니다. 인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 말합니다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
