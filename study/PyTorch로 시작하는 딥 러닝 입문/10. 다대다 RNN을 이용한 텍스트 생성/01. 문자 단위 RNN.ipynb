{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다대다 RNN은 모든 시점의 입력에 대해 모든 시점에 대한 출력을 하는 RNN입니다. 대표적으로 품사 태깅, 개체명 인식 등에 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 문자 단위 RNN(Char RNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN의 입출력의 단위가 단어 레벨이 아니라 문자 레벨로 바꿔 RNN을 구현한다면, 이를 문자 단위 RNN이라고 합니다. RNN 구조 자체가 달라진 것은 아니고, 입출력 단위가 문자로 바뀌었을 뿐입니다. 문자 단위 RNN을 다대다 구조로 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 훈련 데이터 전처리하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현할겁니다. 이는 어떤 의미를 가지진 않지만 RNN의 동작을 이해할 수 있습니다.\n",
    "\n",
    "입력 데이터와 레이블 데이터에 대해서 vocabulary를 만듭니다. 여기서는 문자 집합은 중복을 제거한 문자들의 집합입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기: 5\n"
     ]
    }
   ],
   "source": [
    "input_str = \"apple\"\n",
    "label_str = \"pple!\"\n",
    "char_vocab = sorted(list(set(input_str + label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print(\"문자 집합의 크기: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 문자 집합에는 !, a, e, l, p 총 5개의 문자가 있습니다. 이제 tanh를 정의하겠습니다. 이때 입력은 원-핫 벡터를 사용하며 입력의 크기는 문자 집합의 크기여야만 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 문자 집합에 고유한 정수를 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
    "char_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나중에 예측 결과를 문자로 보기 위해 정수에서 문자를 얻을 수 있는 사전도 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑합니다. 그런데 파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받습니다. 그렇기 때문에 배치 차원을 추가해줍니다. 방법은 둘 중 하나를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[[1, 4, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "\n",
    "# 배치 차원 추가\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "tensor([[1, 4, 4, 3, 2]], dtype=torch.int32)\n",
      "tensor([[4, 4, 3, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "\n",
    "# 텐서 연산인 unsqueeze(0)을 통해 해결할 수도 있음\n",
    "x_data = torch.IntTensor(x_data).unsqueeze(0)\n",
    "y_data = torch.Tensor(y_data).unsqueeze(0)\n",
    "y_data = torch.as_tensor(y_data, dtype=torch.int64) # int64로 변환 / LongTensor를 위해\n",
    "\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "x_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 데이터와 레이블 데이터를 텐서로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELPHI~1\\AppData\\Local\\Temp/ipykernel_5636/2348034151.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기: torch.Size([1, 5, 5])\n",
      "레이블의 크기: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터의 크기: {}\".format(X.shape))\n",
    "print(\"레이블의 크기: {}\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 모델 구현하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 RNN 모델을 구현해보겠습니다. 아래에서 fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "    \n",
    "    def forward(self, x): # 구현한 RNN셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델에 입력을 넣어 출력의 크기를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1, 5, 5)의 크기를 가지는데 각각 배치 차원, 시점, 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐 계산하게 되는데 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5, 5)로 줄어든 것을 확인할 수 있습니다. 이제 레이블 데이터의 크기를 다시 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블 데이터는 (1, 5)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (5)의 크기를 가지게 됩니다. \n",
    "\n",
    "이제 옵티마이저와 손실 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.5310280323028564 prediction:  [[4 4 4 4 2]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  ppppe\n",
      "1 loss:  1.2918808460235596 prediction:  [[4 4 4 4 4]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  ppppp\n",
      "2 loss:  1.0967464447021484 prediction:  [[4 4 2 2 2]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  ppeee\n",
      "3 loss:  0.8909416198730469 prediction:  [[4 4 2 2 2]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  ppeee\n",
      "4 loss:  0.6655368804931641 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "5 loss:  0.4822908937931061 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "6 loss:  0.33621734380722046 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "7 loss:  0.22394892573356628 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "8 loss:  0.14825893938541412 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "9 loss:  0.09916679561138153 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "10 loss:  0.0671209767460823 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "11 loss:  0.04615112394094467 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "12 loss:  0.032418154180049896 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "13 loss:  0.023346420377492905 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "14 loss:  0.017253758385777473 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "15 loss:  0.013075033202767372 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "16 loss:  0.010144185274839401 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "17 loss:  0.008042791858315468 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "18 loss:  0.006504416465759277 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "19 loss:  0.005356288515031338 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "20 loss:  0.004484111908823252 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "21 loss:  0.003810660447925329 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "22 loss:  0.0032829567790031433 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "23 loss:  0.00286375661380589 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "24 loss:  0.002526551950722933 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "25 loss:  0.002252252772450447 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "26 loss:  0.0020267684012651443 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "27 loss:  0.0018395284423604608 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "28 loss:  0.0016827769577503204 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "29 loss:  0.0015504863113164902 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "30 loss:  0.001437930972315371 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "31 loss:  0.001341548515483737 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "32 loss:  0.0012584398500621319 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "33 loss:  0.0011863707331940532 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "34 loss:  0.0011235581478103995 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "35 loss:  0.0010685520246624947 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "36 loss:  0.0010201147524639964 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "37 loss:  0.0009771999903023243 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "38 loss:  0.0009391417843289673 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "39 loss:  0.0009052499081008136 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "40 loss:  0.0008748342515900731 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "41 loss:  0.0008475378272123635 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "42 loss:  0.0008229796658270061 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "43 loss:  0.0008007315918803215 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "44 loss:  0.0007805075729265809 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "45 loss:  0.0007621889235451818 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "46 loss:  0.0007455135928466916 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "47 loss:  0.0007301481673493981 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "48 loss:  0.0007160926470533013 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "49 loss:  0.0007031566346995533 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "50 loss:  0.0006912925164215267 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "51 loss:  0.0006801906274631619 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "52 loss:  0.0006699938094243407 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "53 loss:  0.0006604879163205624 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "54 loss:  0.000651625101454556 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "55 loss:  0.0006433101370930672 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "56 loss:  0.0006355430814437568 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "57 loss:  0.000628204841632396 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "58 loss:  0.0006213430315256119 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "59 loss:  0.0006148862303234637 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "60 loss:  0.0006087390356697142 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "61 loss:  0.0006029016221873462 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "62 loss:  0.0005973739316686988 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "63 loss:  0.0005920844268985093 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "64 loss:  0.0005869854940101504 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "65 loss:  0.0005821962840855122 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "66 loss:  0.0005775737809017301 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "67 loss:  0.0005731419660151005 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "68 loss:  0.0005688768578693271 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "69 loss:  0.0005647070938721299 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "70 loss:  0.0005607517086900771 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "71 loss:  0.0005568678607232869 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "72 loss:  0.0005531507194973528 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "73 loss:  0.0005495050572790205 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "74 loss:  0.000545883143786341 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "75 loss:  0.0005424519185908139 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "76 loss:  0.0005390921724028885 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "77 loss:  0.000535803847014904 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "78 loss:  0.0005325631937012076 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "79 loss:  0.0005294416332617402 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "80 loss:  0.0005263201892375946 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "81 loss:  0.0005233177216723561 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "82 loss:  0.0005203153705224395 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "83 loss:  0.0005174082471057773 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "84 loss:  0.0005145488539710641 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "85 loss:  0.00051168940262869 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "86 loss:  0.0005089252954348922 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "87 loss:  0.0005061372648924589 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "88 loss:  0.0005033969064243138 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "89 loss:  0.000500751833897084 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "90 loss:  0.0004980353405699134 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "91 loss:  0.0004954856703989208 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "92 loss:  0.000492840597871691 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "93 loss:  0.0004903623485006392 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "94 loss:  0.0004877649189438671 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "95 loss:  0.0004852629208471626 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "96 loss:  0.0004828322271350771 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "97 loss:  0.00048035397776402533 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "98 loss:  0.0004779472074005753 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n",
      "99 loss:  0.00047549279406666756 prediction:  [[4 4 3 2 0]] true Y:  tensor([[4, 4, 3, 2, 0]]) prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 통해 배치 차원 제거\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # optimizer에 넣었던 파라미터 업데이트\n",
    "    \n",
    "    # 모델이 실제로 어떻게 예측했는지 확인\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 시점 별 5차원 벡터에 대해 가장 큰 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
