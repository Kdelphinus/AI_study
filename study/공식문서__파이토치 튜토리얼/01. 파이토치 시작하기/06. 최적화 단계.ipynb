{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **모델 매개변수 최적화하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 챕터들에서 모델과 데이터를 준비했습니다. 이제 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트할 차례입니다. 모델을 학습하는 과정으 반복적인 과정을 거칩니다. epoch이라고 흔히 말하는 각 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류(loss)를 계산하고, 매개변수에 대한 오류의 도함수를 수집한 뒤, 경사하강법을 사용하여 이 파라미터들을 최적화(optimize)합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **기본(Pre-requisite) 코드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 장의 코드를 가져오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수입니다. 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있습니다. ([하이퍼파라미터 튜닝 더 알아보기](https://tutorials.pytorch.kr/beginner/hyperparameter_tuning_tutorial.html))\n",
    "\n",
    "학습 시에는 다음과 같은 하이퍼파라미터를 정의합니다.\n",
    "- **epoch** - 데이터셋을 반복하는 횟수\n",
    "- **batch size** - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
    "- **learning rate** - 각 batch 혹은 epoch에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimization Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있습니다. 최적화 단계의 각 반복(iteration)을 epoch라고 합니다.\n",
    "\n",
    "하나의 epoch는 다음 두 부부으로 구성됩니다.\n",
    "- **학습 단계(train loop)** - 학습용 데이터셋을 반복하고 최적의 매개변수로 수렴합니다.\n",
    "- **검증/테스트 단계(validation/test loop)** - 모델 성능이 개선되고 있는지 확인하기 위해 테스트 데이터셋을 반복합니다.\n",
    "\n",
    "학습 단계에서 일어나는 몇 가지 개념들을 간략히 살펴보겠습니다. 최적화 단계는 후에 나올 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높습니다. 손실 함수느 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정하며, 학습 중에 이 값을 최소화하려고 합니다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답(label)을 비교하여 손실을 계산합니다. \n",
    "\n",
    "일반적인 손실함수에는 회귀 문제(regression task)에 사용하는 `nn.MSELoss`(평균 제곱 오차)나 분류에 사용하는 `nn.NLLLoss`(Negative Log Likelihood), 그리고 `nn.LogSoftmax`와 `nn.NLLLoss`를 합친 `nn.CrossEntropyLoss` 등이 있습니다. \n",
    "\n",
    "모델의 출력 logit을 `nn.CrossEntropyLoss`에 전달하여 logit을 정규화하고 예측 오류를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수를 초기화합니다.\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다. 최적화 알고리즘은 이 과정이 수행되는 방식(여기에서는 확률적 경상하강법(SGD; Stochastic Gradient Descent))을 정의합니다. 모든 최적화 절차(logit)는 `optimizer` 객체에 캡슐화(encapsulate)됩니다. 여기서는 SGD 옵티마이저를 사용하고 있으며, PyTorch에는 ADAM이나 RMSProp와 같은 다른 종류의 모델과 데이터에서 더 잘 작동하는 다양한 옵티마이저가 있습니다. \n",
    "\n",
    "학습하려는 모델의 매개변수와 학습률 하이퍼파라미터를 등록하여 옵티마이저를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 단계에서 최적화는 세 단계로 이루어집니다.\n",
    "- `optimizer.zero_grad()`를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정합니다.\n",
    "- `loss.backward()`를 호출하여 예측 손실(prediction loss)을 역전파합니다. PyTorch는 각 매개벼수에 대한 손실의 변화도를 저장합니다.\n",
    "- 변화도를 계산한 뒤에는 `optimizer.step()`을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **전체 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화 코드를 반복하여 수행하는 `train_loop`와 테스트 데이터로 모델의 성능을 측정하는 `test_loop`를 정의했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # prediction과 loss 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backprapagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수와 옵티마이저를 초기화하고 `train_loop`와 `test_loop`에 전달합니다. 모델의 성능 향상을 알아보기 위해 자유롭게 epoch 수를 증가시켜 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------------\n",
      "loss: 2.172620 [    0/60000]\n",
      "loss: 2.177859 [ 6400/60000]\n",
      "loss: 2.117976 [12800/60000]\n",
      "loss: 2.148351 [19200/60000]\n",
      "loss: 2.096275 [25600/60000]\n",
      "loss: 2.030796 [32000/60000]\n",
      "loss: 2.085051 [38400/60000]\n",
      "loss: 2.001935 [44800/60000]\n",
      "loss: 2.004154 [51200/60000]\n",
      "loss: 1.951130 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 1.937160 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------------\n",
      "loss: 1.956720 [    0/60000]\n",
      "loss: 1.947445 [ 6400/60000]\n",
      "loss: 1.827912 [12800/60000]\n",
      "loss: 1.885595 [19200/60000]\n",
      "loss: 1.765440 [25600/60000]\n",
      "loss: 1.706828 [32000/60000]\n",
      "loss: 1.758420 [38400/60000]\n",
      "loss: 1.640265 [44800/60000]\n",
      "loss: 1.655632 [51200/60000]\n",
      "loss: 1.571076 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.567236 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------------\n",
      "loss: 1.618651 [    0/60000]\n",
      "loss: 1.597743 [ 6400/60000]\n",
      "loss: 1.435341 [12800/60000]\n",
      "loss: 1.522757 [19200/60000]\n",
      "loss: 1.384724 [25600/60000]\n",
      "loss: 1.377697 [32000/60000]\n",
      "loss: 1.410433 [38400/60000]\n",
      "loss: 1.316537 [44800/60000]\n",
      "loss: 1.345190 [51200/60000]\n",
      "loss: 1.263876 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.275058 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------------\n",
      "loss: 1.345140 [    0/60000]\n",
      "loss: 1.338692 [ 6400/60000]\n",
      "loss: 1.163554 [12800/60000]\n",
      "loss: 1.277448 [19200/60000]\n",
      "loss: 1.141944 [25600/60000]\n",
      "loss: 1.169814 [32000/60000]\n",
      "loss: 1.199058 [38400/60000]\n",
      "loss: 1.124496 [44800/60000]\n",
      "loss: 1.158129 [51200/60000]\n",
      "loss: 1.093292 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.101691 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------------\n",
      "loss: 1.167736 [    0/60000]\n",
      "loss: 1.182152 [ 6400/60000]\n",
      "loss: 0.993088 [12800/60000]\n",
      "loss: 1.132437 [19200/60000]\n",
      "loss: 0.995942 [25600/60000]\n",
      "loss: 1.034285 [32000/60000]\n",
      "loss: 1.073733 [38400/60000]\n",
      "loss: 1.006679 [44800/60000]\n",
      "loss: 1.040352 [51200/60000]\n",
      "loss: 0.989709 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.992660 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------------\n",
      "loss: 1.047322 [    0/60000]\n",
      "loss: 1.083183 [ 6400/60000]\n",
      "loss: 0.879012 [12800/60000]\n",
      "loss: 1.038319 [19200/60000]\n",
      "loss: 0.904655 [25600/60000]\n",
      "loss: 0.939461 [32000/60000]\n",
      "loss: 0.992404 [38400/60000]\n",
      "loss: 0.930714 [44800/60000]\n",
      "loss: 0.959576 [51200/60000]\n",
      "loss: 0.920709 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.918519 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------------\n",
      "loss: 0.959511 [    0/60000]\n",
      "loss: 1.014788 [ 6400/60000]\n",
      "loss: 0.797316 [12800/60000]\n",
      "loss: 0.972223 [19200/60000]\n",
      "loss: 0.843375 [25600/60000]\n",
      "loss: 0.869507 [32000/60000]\n",
      "loss: 0.935111 [38400/60000]\n",
      "loss: 0.879416 [44800/60000]\n",
      "loss: 0.900759 [51200/60000]\n",
      "loss: 0.870191 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.864553 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------------\n",
      "loss: 0.891596 [    0/60000]\n",
      "loss: 0.963234 [ 6400/60000]\n",
      "loss: 0.735124 [12800/60000]\n",
      "loss: 0.923006 [19200/60000]\n",
      "loss: 0.799337 [25600/60000]\n",
      "loss: 0.816087 [32000/60000]\n",
      "loss: 0.891948 [38400/60000]\n",
      "loss: 0.843484 [44800/60000]\n",
      "loss: 0.856382 [51200/60000]\n",
      "loss: 0.830833 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.823166 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------------\n",
      "loss: 0.836782 [    0/60000]\n",
      "loss: 0.921855 [ 6400/60000]\n",
      "loss: 0.685475 [12800/60000]\n",
      "loss: 0.884372 [19200/60000]\n",
      "loss: 0.765611 [25600/60000]\n",
      "loss: 0.774219 [32000/60000]\n",
      "loss: 0.857407 [38400/60000]\n",
      "loss: 0.816801 [44800/60000]\n",
      "loss: 0.821581 [51200/60000]\n",
      "loss: 0.798847 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.790097 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------------\n",
      "loss: 0.791066 [    0/60000]\n",
      "loss: 0.886650 [ 6400/60000]\n",
      "loss: 0.644883 [12800/60000]\n",
      "loss: 0.853518 [19200/60000]\n",
      "loss: 0.738393 [25600/60000]\n",
      "loss: 0.740915 [32000/60000]\n",
      "loss: 0.828158 [38400/60000]\n",
      "loss: 0.795786 [44800/60000]\n",
      "loss: 0.793374 [51200/60000]\n",
      "loss: 0.771908 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.762686 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------------\n",
      "loss: 2.306897 [    0/60000]\n",
      "loss: 0.553359 [ 6400/60000]\n",
      "loss: 0.394523 [12800/60000]\n",
      "loss: 0.502762 [19200/60000]\n",
      "loss: 0.426639 [25600/60000]\n",
      "loss: 0.432372 [32000/60000]\n",
      "loss: 0.378134 [38400/60000]\n",
      "loss: 0.520631 [44800/60000]\n",
      "loss: 0.478595 [51200/60000]\n",
      "loss: 0.526594 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.444078 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------------\n",
      "loss: 0.296570 [    0/60000]\n",
      "loss: 0.351552 [ 6400/60000]\n",
      "loss: 0.303195 [12800/60000]\n",
      "loss: 0.389461 [19200/60000]\n",
      "loss: 0.381743 [25600/60000]\n",
      "loss: 0.365842 [32000/60000]\n",
      "loss: 0.320844 [38400/60000]\n",
      "loss: 0.507298 [44800/60000]\n",
      "loss: 0.392264 [51200/60000]\n",
      "loss: 0.487467 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.393266 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------------\n",
      "loss: 0.218889 [    0/60000]\n",
      "loss: 0.329521 [ 6400/60000]\n",
      "loss: 0.235537 [12800/60000]\n",
      "loss: 0.324807 [19200/60000]\n",
      "loss: 0.425427 [25600/60000]\n",
      "loss: 0.343203 [32000/60000]\n",
      "loss: 0.270144 [38400/60000]\n",
      "loss: 0.441569 [44800/60000]\n",
      "loss: 0.329820 [51200/60000]\n",
      "loss: 0.400434 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.373342 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------------\n",
      "loss: 0.195681 [    0/60000]\n",
      "loss: 0.308169 [ 6400/60000]\n",
      "loss: 0.219687 [12800/60000]\n",
      "loss: 0.316354 [19200/60000]\n",
      "loss: 0.349675 [25600/60000]\n",
      "loss: 0.348589 [32000/60000]\n",
      "loss: 0.227350 [38400/60000]\n",
      "loss: 0.381670 [44800/60000]\n",
      "loss: 0.291797 [51200/60000]\n",
      "loss: 0.330896 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.375905 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------------\n",
      "loss: 0.218227 [    0/60000]\n",
      "loss: 0.260816 [ 6400/60000]\n",
      "loss: 0.210851 [12800/60000]\n",
      "loss: 0.246978 [19200/60000]\n",
      "loss: 0.377867 [25600/60000]\n",
      "loss: 0.287946 [32000/60000]\n",
      "loss: 0.225126 [38400/60000]\n",
      "loss: 0.352526 [44800/60000]\n",
      "loss: 0.256219 [51200/60000]\n",
      "loss: 0.300559 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.344651 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------------\n",
      "loss: 0.163590 [    0/60000]\n",
      "loss: 0.184458 [ 6400/60000]\n",
      "loss: 0.212696 [12800/60000]\n",
      "loss: 0.250566 [19200/60000]\n",
      "loss: 0.355669 [25600/60000]\n",
      "loss: 0.296925 [32000/60000]\n",
      "loss: 0.212389 [38400/60000]\n",
      "loss: 0.341883 [44800/60000]\n",
      "loss: 0.257284 [51200/60000]\n",
      "loss: 0.317831 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.350515 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------------\n",
      "loss: 0.178739 [    0/60000]\n",
      "loss: 0.239420 [ 6400/60000]\n",
      "loss: 0.201020 [12800/60000]\n",
      "loss: 0.229501 [19200/60000]\n",
      "loss: 0.366750 [25600/60000]\n",
      "loss: 0.274579 [32000/60000]\n",
      "loss: 0.209040 [38400/60000]\n",
      "loss: 0.329948 [44800/60000]\n",
      "loss: 0.245623 [51200/60000]\n",
      "loss: 0.299096 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.342600 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------------\n",
      "loss: 0.170769 [    0/60000]\n",
      "loss: 0.187604 [ 6400/60000]\n",
      "loss: 0.211793 [12800/60000]\n",
      "loss: 0.199120 [19200/60000]\n",
      "loss: 0.300572 [25600/60000]\n",
      "loss: 0.266402 [32000/60000]\n",
      "loss: 0.234988 [38400/60000]\n",
      "loss: 0.265584 [44800/60000]\n",
      "loss: 0.240714 [51200/60000]\n",
      "loss: 0.262278 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.344774 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------------\n",
      "loss: 0.154707 [    0/60000]\n",
      "loss: 0.201978 [ 6400/60000]\n",
      "loss: 0.183435 [12800/60000]\n",
      "loss: 0.178443 [19200/60000]\n",
      "loss: 0.322772 [25600/60000]\n",
      "loss: 0.248073 [32000/60000]\n",
      "loss: 0.250773 [38400/60000]\n",
      "loss: 0.249045 [44800/60000]\n",
      "loss: 0.214721 [51200/60000]\n",
      "loss: 0.264844 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.369743 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------------\n",
      "loss: 0.135102 [    0/60000]\n",
      "loss: 0.145326 [ 6400/60000]\n",
      "loss: 0.165153 [12800/60000]\n",
      "loss: 0.188013 [19200/60000]\n",
      "loss: 0.285733 [25600/60000]\n",
      "loss: 0.214894 [32000/60000]\n",
      "loss: 0.204233 [38400/60000]\n",
      "loss: 0.202793 [44800/60000]\n",
      "loss: 0.182720 [51200/60000]\n",
      "loss: 0.220565 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.361039 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
